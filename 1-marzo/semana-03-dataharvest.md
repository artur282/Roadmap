# ğŸ•·ï¸ Semana 03 â€” DataHarvest

> **Web scraper inteligente con Selenium, Pandas y anÃ¡lisis automatizado de datos**

| Campo              | Detalle             |
| ------------------ | ------------------- |
| ğŸ“… Fechas          | 21-22 de marzo 2026 |
| ğŸ·ï¸ CategorÃ­a       | Backend Foundations |
| â±ï¸ Tiempo estimado | 10-12 horas         |
| ğŸ“Š Dificultad      | â­â­â­ Intermedio   |

---

## ğŸ¯ DescripciÃ³n

DataHarvest es un web scraper robusto y Ã©tico que recolecta datos de fuentes pÃºblicas (por ejemplo, datos de mercado, empleos tech, o noticias), los procesa con Pandas y los almacena de forma estructurada. Incluye manejo de errores, retries, rotaciÃ³n de user-agents, y exportaciÃ³n a mÃºltiples formatos.

Este proyecto sienta las bases para los proyectos de datos del mes de mayo.

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Scheduler/CLI            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Scraper Engine  â”‚
       â”‚  (Selenium +     â”‚
       â”‚   requests)      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚            â”‚            â”‚
â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Parserâ”‚  â”‚Validatorâ”‚  â”‚Cleaner â”‚
â”‚(BS4) â”‚  â”‚(Schema) â”‚  â”‚(Pandas)â”‚
â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
   â”‚            â”‚            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚      Data Store          â”‚
   â”‚  (PostgreSQL + CSV/JSON) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Features

### Scraping

- [ ] Motor de scraping con Selenium (pÃ¡ginas dinÃ¡micas)
- [ ] Fallback a requests + BeautifulSoup (pÃ¡ginas estÃ¡ticas)
- [ ] RotaciÃ³n de user-agents
- [ ] Manejo de rate limiting y delays Ã©ticos
- [ ] Retry con backoff exponencial
- [ ] Respeto de robots.txt

### Procesamiento de datos

- [ ] Limpieza y normalizaciÃ³n con Pandas
- [ ] ValidaciÃ³n de schema de datos
- [ ] DeduplicaciÃ³n inteligente
- [ ] Transformaciones personalizables
- [ ] DetecciÃ³n de anomalÃ­as bÃ¡sicas

### Almacenamiento y exportaciÃ³n

- [ ] Persistencia en PostgreSQL
- [ ] ExportaciÃ³n a CSV, JSON, Excel
- [ ] Historial de ejecuciones
- [ ] Logs detallados de cada run

### CLI

- [ ] Interfaz CLI para ejecutar scrapers
- [ ] ConfiguraciÃ³n por YAML/TOML
- [ ] Modo dry-run para testing
- [ ] Reporte de resultados en consola

---

## ğŸ› ï¸ Stack tÃ©cnico

| TecnologÃ­a         | PropÃ³sito                         |
| ------------------ | --------------------------------- |
| **Selenium**       | Scraping de pÃ¡ginas dinÃ¡micas     |
| **BeautifulSoup4** | Parsing HTML                      |
| **Pandas**         | Procesamiento y limpieza de datos |
| **PostgreSQL**     | Almacenamiento persistente        |
| **SQLAlchemy**     | ORM                               |
| **Typer**          | Interfaz CLI                      |
| **Docker Compose** | Infraestructura                   |
| **pytest**         | Testing                           |

---

## ğŸ“ Estructura del proyecto

```
dataharvest/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                 # Entry point CLI
â”‚   â”œâ”€â”€ config.py              # ConfiguraciÃ³n
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ base.py            # Clase base de scraper
â”‚   â”‚   â”œâ”€â”€ jobs_scraper.py    # Scraper de empleos tech
â”‚   â”‚   â””â”€â”€ news_scraper.py    # Scraper de noticias
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ cleaner.py         # Limpieza de datos
â”‚   â”‚   â”œâ”€â”€ validator.py       # ValidaciÃ³n de schema
â”‚   â”‚   â””â”€â”€ transformer.py     # Transformaciones
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ database.py        # PostgreSQL
â”‚   â”‚   â””â”€â”€ exporters.py       # CSV, JSON, Excel
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ user_agents.py     # RotaciÃ³n UA
â”‚       â””â”€â”€ retry.py           # Retry logic
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_scrapers.py
â”‚   â”œâ”€â”€ test_processors.py
â”‚   â””â”€â”€ fixtures/              # HTML de ejemplo para tests
â”œâ”€â”€ config/
â”‚   â””â”€â”€ scrapers.yml           # ConfiguraciÃ³n de scrapers
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ—“ï¸ Plan del fin de semana

### SÃ¡bado

| Hora           | Actividad                                          |
| -------------- | -------------------------------------------------- |
| ğŸŒ… 9:00-10:00  | Setup del proyecto, Docker, dependencias           |
| ğŸŒ… 10:00-12:00 | Clase base de scraper + Selenium setup             |
| ğŸŒ 12:00-13:00 | Primer scraper funcional (empleos tech)            |
| ğŸŒ 14:00-16:00 | Procesadores: limpieza, validaciÃ³n, transformaciÃ³n |
| ğŸŒ† 16:00-18:00 | Almacenamiento en PostgreSQL + exportadores        |

### Domingo

| Hora           | Actividad                                   |
| -------------- | ------------------------------------------- |
| ğŸŒ… 9:00-10:30  | CLI con Typer + configuraciÃ³n YAML          |
| ğŸŒ… 10:30-12:00 | Segundo scraper (noticias tech)             |
| ğŸŒ 13:00-14:30 | Tests con fixtures HTML                     |
| ğŸŒ 14:30-16:00 | User-agent rotation, retries, rate limiting |
| ğŸŒ† 16:00-17:00 | README y documentaciÃ³n                      |

---

## âœ… DefiniciÃ³n de "hecho"

- [ ] Al menos 2 scrapers funcionales
- [ ] Pipeline: scraping â†’ limpieza â†’ validaciÃ³n â†’ almacenamiento
- [ ] ExportaciÃ³n a CSV y JSON
- [ ] CLI funcional con comandos claros
- [ ] Tests con HTML fixtures (sin depender de internet)
- [ ] Manejo de errores y retries
- [ ] README con instrucciones y ejemplos

---

## ğŸ’¼ Lo que demuestra al reclutador

| Habilidad    | Evidencia                            |
| ------------ | ------------------------------------ |
| Web scraping | Selenium + BS4 + manejo robusto      |
| Datos        | Pandas, limpieza, validaciÃ³n         |
| DiseÃ±o       | PatrÃ³n base class, plugins, SOLID    |
| Ã‰tica        | Respeto de robots.txt, rate limiting |
| Testing      | Tests sin dependencias externas      |
